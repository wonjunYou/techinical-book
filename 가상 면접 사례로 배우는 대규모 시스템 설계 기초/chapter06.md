# 6장 - 키-값 저장소 설계

## 키-값 저장소란?

- 키-값 저장소(key-value store)는 비 관계형 데이터베이스
- 고유 식별자(identifier)를 키로 갖는다.
- 키의 사례는 “last_logged_int_at” 처럼 텍스트 키와 “253DDEC4” 처럼 해시 키가 있다.
- 아마존 다이나모, memcached, redis

키의 길이는 성능상 이유로 짧게 유지하면 좋음.

## 문제 이해 및 설계 범위 확정

- 키-값 쌍의 크기는 10KB 이하
- 큰 데이터 저장할 수 있어야 한다.
- 높은 가용성(장애 시에도 빠르게 응답)
- 높은 규모 확장성(트래픽 양에 따라 scale-out 조절 가능)
- 데이터 일관성 수준은 조정이 가능해야 함.
- 응답 지연시간(latency)은 짧아야 함.

## 단일 키-값 저장소

* 한 대의 서버만 사용하는 키-값 저장소를 설계한다면?
  * 가장 직관적인 방법으로, 키-값 쌍 전부를 메모리에 해시 테이블로 저장하는 것.
    * 빠른 속도는 보장되나, but 모든 데이터를 메모리 안에 두는 것은 불가능할수도 있다.

해당 문제점을 해결하기 위해, 다음과 같은 개선책이 있다.
* 데이터 압축
* 자주 쓰이는 데이터만 메모리에 두고 나머지는 디스크에 저장

단일 키-값 저장소는 한계가 있으므로, 분산 저장소를 고려해야 함.

## 분산 키-값 저장소

분산 시스템 설계에는 CAP 이론에 대한 정리가 필요하다.

## CAP 정리

아래 C, A, P 중 3가지 속성을 모두 충족하는 것은 불가능. 
모든 DB가 셋 중에 **두 가지 속성을 충족**시킨다.

- 일관성(consistency)
    - 클라이언트는 어떤 노드에 접속 했느냐에 관계 없이 항상 같은 데이터를 읽어야 한다.
- 가용성(availability)
    - 클라이언트는 일부 노드에 장애가 발생해도 항상 응답을 받아야 한다.
    - scale-out, 로드 밸런싱, 로깅 및 모니터링, 쿠버네티스 등.
- 파티션 감내성(partition tolerance)
    - 두 노드 사이 통신 장애가 발생해도, 시스템은 계속 동작해야 한다.
    - 데이터 복제(Replication), 분할 인식 알고리즘 등.

- CP 시스템
    - 일관성과 파티션 감내를 지원, 가용성을 희생
- AP 시스템
    - 가용성과 파티션 감내를 지원, 일관성을 희생
- CA 시스템(존재하지 않음)
    - 분산시스템은 반드시 파티션 문제를 감내할 수 있어야 한다.
    - 실세계에 CA 시스템은 존재하지 않는다.

## 이상적 상태
* 네트워크가 파티션되는 상황은 절대로 일어나지 않는다
* n1에 기록된 데이터가 자동적으로 n2와 n3에 복제됨
* 데이터 일관성과 가용성 모두 만족

## 실세계 분산 시스템
* 분산 시스템은 파티션 문제를 피할 수 없다, 일관성과 가용성 중 하나를 선택하자.
* 만약 n3에 장애 발생 시, n1, n2에 기록한 데이터가 n3에는 전달되지 않으므로 일관성이 충족 x.

**만약 일관성을 선택하면? (CP 시스템)**
* 데이터 불일치 문제를 고려, n1과 n2 에 쓰기 연산을 중단한다.
* 뱅킹 시스템은 일관성이 중요하므로, 문제 해결 전까지 클라이언트에 오류를 반환한다.

## 시스템 핵심 컴포넌트

- 데이터 파티션
- 데이터 다중화(replication)
- 일관성(consistency)
- 일관성 불일치 해소(inconsistency resolution)
- 장애 처리
- 시스템 아키텍처 다이어그램
- 쓰기 경로(write path)
- 읽기 경로(read path)

## 데이터 파티션

* 대규모 시스템의 경우 데이터를 한 대 서버에 모두 넣는 것은 불가.

* 데이터를 파티셔닝할 때 아래 두 요소를 고려하자
  * 데이터를 여러 서버에 고르게 분산 가능한지
  * 노드가 추가, 삭제 시 데이터 이동이 최소화되는가?

안정 해시(consistent hash)는 이런 문제를 푸는 적합한 기술이다.

## 데이터 다중화(replication)

* 높은 가용성을 위해 데이터를 N개 서버에 비동기적으로 다중화할 필요가 있음.
  * N은 튜닝 가능한 값이다.
  
  
* N개 서버를 선정하는 방법?
  * 키를 해시 링 위에 배치, 시계 방향으로 링을 순회하면서 만나는 첫 N개 서버에 데이터 사본을 보관한다.

## 데이터 일관성
여러 노드에 다중화된 데이터는 어떻게 동기화가 이루어지는가?
* 정족수 합의(Quorum Consensus) 프로토콜
  * 읽기, 쓰기 연산 모두 일관성을 보장.

아래 3가지 인자가 존재한다.
- 쓰기가 성공했다고 인정하기 위해, 몇 개의 노드에 쓰기 연산이 완료되었다고 응답받아야 할까?
    - 이것을 **쓰기 연산에 대한 정족수(W)**라고 한다.
- 읽기가 성공했다고 인정하기 위해, 몇 개의 노드에 읽기 연산이 완료되었다고 응답받아야 할까?
    - 이것을 **읽기 연산에 대한 정족수(R)**라고 한다.
- 사본의 수(N)

N, W, R의 값을 어떻게 정할까? 이것은 상황에 따라 다르며, 다음과 같이 분류할 수 있다.

- R=1, W=N: 빠른 읽기.
- W=1, R=N: 빠른 쓰기
- W+R>N 강한 일관성이 보장, 속도보다 일관성.

### 일관성 모델

비즈니스의 특성과 요구사항에 따라서 일관성의 정도를 결정해야 한다.

- 강한 일관성(strong consistency)
    - 모든 읽기는 가장 최근에 갱신된 결과를 반환한다.(오래된 데이터 조회 x)
- 약한 일관성(weak consistency)
    - 모든 읽기가 가장 최근에 갱신된 결과를 반환하지 못 할 수도 있다.
- 최종 일관성(eventual consistency)
    - 약한 일관성 중 하나의 형태다
    - 갱신 결과가 결국에는 모든 사본에 동기화(반영)된다.

- 강한 일관성의 경우 고가용성 시스템에는 적합하지 않음.
  - 일관성 보장을 위해 대기하는 동안 새로운 요청의 처리가 중단되기 때문
  - DynamoDB, 카산드라가 이에 해당.

## 비 일관성 해소기법

데이터를 다중화하면 가용성은 높아지나 **사본 간 일관성 보장이 어렵다.**

이를 해결하기 위해 버저닝과 벡터 시계라는 기법이 있다.


### 데이터 버저닝(versioning)
* 데이터를 변경할 때마다 해당 데이터의 새로운 버전을 만드는 것
* 각 버전 데이터는 불변(immutable)

* 하지만, 두 클라이언트가 동시에 같은 키 값으로 두 개의 사본 저장소에 쓰기 작업을 하면 충돌이 발생함.

이를 해결하기 위한 것이 벡터 시계(vector clock)이다.

### 백터 시계
- [서버, 버전]의 순서쌍을 데이터에 매단 것.
- 벡터 시계의 표현법
    - D ( [S1, V1], [S2, V2], ... [SN, VN] )
- 만약, 데이터를 S1에 등록 한다면, 다음 작업 중 하나를 수행한다.
    - [SI, VI]가 있으면 VI을 하나 증가시킨다.
    - 없다면, 새 항목 [S1, V1]을 생성한다.

다음은 벡터 시계의 사용 시나리오다.

- 서버1이 쓰기 연산 → D1[S1, V1]
- 서버1이 쓰기 연산 → D1[S1, V2]
- 서버2와 서버3이 동시에 쓰기 연산
    - D3([S1, V2], [S2, V1])
    - D3([S1, V2], [S3, V1])
- 서버1이 읽기를 하려고 하는데 충돌이 발생함, 서버 1은 충돌 해소위해 새로 기록
    - D5([S1, V3], [S2, V1], [S3, V1])

- 이렇게 하면, 버전 Y에 포함된 모든 구성요소의 값이 X에 포함된 모든 구성요소 값보다 같거나 큰지만 보면 된다.
  - 만약 그렇다면, 두 데이터 사이의 충돌은 없다.


**벡터 시계의 단점**
- 충돌 감지 및 해소로직을 클라이언트가 수행해야 한다.
- [서버: 버전] 순서쌍의 개수가 빠르게 증가한다.

## 장애 처리 - 장애 감지
어떻게 장애를 감지하는가? 

- 멀티캐스팅(multicasting) 채널 구축
    - 각 서버를 모두 연결한다.
    - 서버가 많아질 수록 비효율적이다.
- 가십 프로토콜(gossip protocol)
    - 멤버십 목록(membership list)를 저장한다.
    - 멤버십 목록에는 각 멤버(서버)ID와 해당 멤버의 박동 카운터(heartbeat counter)를 갖는다.
    - 각 노드는 주기적으로 자신의 박동 카운터를 증가한다.
    - 각 노드는 무작위로 선정하여 주기적으로 자기 박동 카운터 값을 보낸다.
    - 박동 카운터 값을 받은 노드는 멤버십 목록을 최신으로 갱신한다.
    - **어떤 멤버의 박동 카운터 값이 지정된 시간 동안 갱신되지 않을 경우, 해당 멤버를 장애 상태로 간주한다.**

## 장애 처리

* 장애를 어떻게 처리할 것인가?
  * 엄격한 정족수 접근법을 쓰면 읽기와 쓰기를 금지해야 함.
  * 느슨한 정족수 접근법을 쓰면 가용성을 높인다.
    * 쓰기 연산을 수행할 W개 서버, 읽기 연산을 수행할 R개의 건강한 서버를 해시 링에서 고름.
      * 이때 장애 서버는 무시한다.

* 네트워크나 서버 문제일 경우 단순히 다른 서버가 잠시 맡아 처리한다.
  * 장애 서버가 복구될 때 변경사항을 일괄 반영하면 된다.

### 영구 장애 처리 - 반-엔트로피 프로토콜을 구현

- 사본들을 비교하여 최신 버전으로 갱신하는 방법
- 사본 간 일관성이 망가진 상태를 탐지하고 전송 데이터양을 줄여야한다.
  - 머클 트리(해시트리)를 사용하자.
    - 각 노드에 그 자식 노드들에 보관된 값의 해시(자식 노드가 leaf 노드일 경우)
    - 또는, 자식 노드들의 레이블로부터 계산된 해시 값을 레이블로 붙여두는 트리

키 공간이 1부터 12까지 구성될 경우,
1. 키 공간을 버킷으로 나눈다.
2. 버킷에 포함된 각각의 키에 균등 분포 해시 함수를 이용하여 해시 값을 계산한다.
3. 버킷별로 해시값을 계산한다. 해당 해시 값을 레이블로 갖는 노드를 생성한다.
4. 자식 노드의 레이블로부터 새로운 해시 값을 계산하여 이진 트리를 상향식으로 만들어 나간다.

## 데이터 센터 장애 처리

데이터 센터 장애는 여러 데이터 센터에 다중화하는 것이 필수.

## 시스템 아키텍처 다이어그램

아키텍처의 주된 기능은 다음과 같음.
- 클라이언트는 API( get(key), put(key, value) ) 와 통신한다.
- 중재자는 클라이언트에게 키-값 저장소에 대해 proxy 역할을 한다.
- 노드는 안정 해시의 해시 링 위에 분포된다.
- 노드를 자동으로 추가 삭제할 수 있도록 시스템은 분산된다.
- 데이터는 여러 노드에 다중화 된다.
- SPOF(Single Point of Failure)는 존재하지 않는다.(모든 노드가 동등함)

## 카산드라의 WRITE, READ 예시
쓰기 요청은 아래 순서를 통해 처리한다.

1. 쓰기 요청을 커밋 로그 파일에 기록
2. 데이터를 메모리 캐시에 기록
3. 메모리 캐시가 가득차거나, 임계치에 도달 시 데이터를 디스크의 SSTable에 저장
   * <키,값> 순서쌍을 정렬된 리스트 형태로 관리하는 테이블


읽기 요청은 아래 순서를 통해 처리한다.
1. 데이터가 메모리 캐시에 있는지 살핀다.
2. 데이터가 메모리에 없다면? 다른 곳에 있으므로, 디스크(SSTable) 에서 가져와야 한다.
3. 블룸필터(Bloom Filter)를 검사한다.
4. 블룸필터는 어떤 SSTable에 키가 보관됐는지 검사한다.
5. SSTable에서 데이터를 가져와 클라이언트에게 반환한다.

## Summary

분산 키-값 저장소의 문제들과 해당 문제에 대한 기술들을 정리하면 다음과 같다.

| 목표/문제 | 기술 |
| --- | --- |
| 대규모 데이터 저장 | 안정 해시를 사용하여 서버에 부하 분산 |
| 읽기 연산에 대한 가용성 보장 | 여러 데이터센터에 다중화 |
| 쓰기 연산에 대한 가용성 보장 | 버저닝 및 벡터 시계를 사용하여 충돌 해소 |
| 데이터 파티션 | 안정 해시 |
| 점진적 규모 확장성 | 안정 해시 |
| 다양성(heterogeneity) | 안정 해시 |
| 조절 가능한 데이터 일관성 | 정족수 합의(quorum consensus) |
| 일시적 장애 처리 | 느슨한 정족수 프로토콜 및 단서 후 임시 위탁 |
| 영구적 장애 처리 | 머클 트리 |
| 데이터 센터 장애 대응 | 여러 데이터센터에 다중화 |